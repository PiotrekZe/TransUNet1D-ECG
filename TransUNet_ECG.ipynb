{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5f8c0-95e3-46c7-93e9-96a8bff1007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import wfdb\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933523d3-3213-4b96-9b03-9986bc461eac",
   "metadata": {},
   "source": [
    "# Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6a935-c0e4-4f85-85a6-c2c6acfbf560",
   "metadata": {},
   "source": [
    "### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24ecda-44d3-4691-ae33-f3f7e4304be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1, bias=False, padding=None, activation=nn.SiLU,):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers = []\n",
    "        if padding is None:\n",
    "            padding = kernel_size // 2\n",
    "            \n",
    "        layers.extend(\n",
    "            [\n",
    "                nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, groups=groups,),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            ]\n",
    "        )\n",
    "        if activation is not None:\n",
    "            if \"inplace\" in activation.__init__.__code__.co_varnames:\n",
    "                layers.extend([activation(inplace=True)])\n",
    "            else:\n",
    "                layers.extend([activation()])\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class SEBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=24):\n",
    "        super(SEBlock1D, self).__init__()\n",
    "        if in_channels == 32:\n",
    "            reduction=4\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x)\n",
    "        y = y.view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expansion_factor):\n",
    "        super(MBConv, self).__init__()\n",
    "        hidden_dim = in_channels * expansion_factor\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "\n",
    "        layers = []\n",
    "        if expansion_factor != 1:\n",
    "            layers.extend(\n",
    "                [\n",
    "                    ConvBlock(in_channels=in_channels, out_channels=hidden_dim, kernel_size=1, stride=1, groups=1,)\n",
    "                ]\n",
    "            )\n",
    "        layers.extend(\n",
    "            [\n",
    "                ConvBlock(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=kernel_size, stride=stride, groups=hidden_dim,),\n",
    "                SEBlock1D(in_channels=hidden_dim),\n",
    "                ConvBlock(in_channels=hidden_dim, out_channels=out_channels, kernel_size=1, stride=1, groups=1, activation=None,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        if self.use_residual:\n",
    "            for i in self.conv:\n",
    "                x = i(x)\n",
    "            return x + input\n",
    "        else:\n",
    "            for i in self.conv:\n",
    "                x = i(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374b8d1-b2ab-4485-8d1b-d792f370abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(EfficientNetB0, self).__init__()\n",
    "\n",
    "        self.stages = [\n",
    "            # [Operator(f), Channels(c), Layers(l), Kernel(k), Stride(s), Expansion(exp)]\n",
    "            [ConvBlock, 32, 1, 3, 2, 1],\n",
    "            [MBConv, 16, 1, 3, 1, 1],\n",
    "            [MBConv, 24, 2, 3, 2, 6],\n",
    "            [MBConv, 40, 2, 5, 2, 6],\n",
    "            [MBConv, 80, 3, 3, 2, 6],\n",
    "            [MBConv, 112, 3, 5, 1, 6],\n",
    "            [MBConv, 192, 4, 5, 2, 6],\n",
    "            [MBConv, 320, 1, 3, 1, 6],\n",
    "            [ConvBlock, 1280, 1, 1, 1, 0]\n",
    "        ]\n",
    "        layers = []\n",
    "        last_channel = input_channels\n",
    "        for i in self.stages:\n",
    "            block, channel, num_layers, kernel, stride, expansion = i\n",
    "            if block == ConvBlock:\n",
    "                layers.extend([block(in_channels=last_channel,out_channels=channel,kernel_size=kernel,stride=stride,)])\n",
    "                last_channel = channel\n",
    "            elif block == MBConv:\n",
    "                for j in range(num_layers):\n",
    "                    if j == 0:\n",
    "                        layers.extend([block(in_channels=last_channel,out_channels=channel,kernel_size=kernel,stride=stride,expansion_factor=expansion,)])\n",
    "                    else:\n",
    "                        layers.extend([block(in_channels=last_channel,out_channels=channel,kernel_size=kernel,stride=1,expansion_factor=expansion,)])\n",
    "                    last_channel = channel\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, return_layers=True):\n",
    "        features = []\n",
    "        for i, block in enumerate(self.conv):\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "\n",
    "\n",
    "        return x, [features[5], features[3], features[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055b898-c02d-49e7-9135-570e153da064",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea0f6d2-b8b0-466b-b491-0db75cdc8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05ed63-c1bf-4a7a-b86b-d85fe3be6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, block, layers, in_channels):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        features.append(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        features.append(x)\n",
    "        x = self.layer2(x)\n",
    "        features.append(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        return x, features[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae5053-bce5-4e06-9a56-1e8f390bf88b",
   "metadata": {},
   "source": [
    "### MobileNetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb53dea-52c9-40a0-b120-86eaef42672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1, bias=False, padding=None, activation=nn.SiLU,):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers = []\n",
    "        if padding is None:\n",
    "            padding = kernel_size // 2\n",
    "        layers.extend(\n",
    "            [\n",
    "                nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, groups=groups,),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            ]\n",
    "        )\n",
    "        if activation is not None:\n",
    "            if \"inplace\" in activation.__init__.__code__.co_varnames:\n",
    "                layers.extend([activation(inplace=True)])\n",
    "            else:\n",
    "                layers.extend([activation()])\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expansion_factor):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(in_channels * expansion_factor)\n",
    "        self.use_res_connect = self.stride == 1 and in_channels == out_channels\n",
    "\n",
    "        if expansion_factor == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv1d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv1d(hidden_dim, out_channels, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv1d(in_channels, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv1d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv1d(hidden_dim, out_channels, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNet1D(nn.Module):\n",
    "    def __init__(self, input_channels, width_mult=1.0):\n",
    "        super(MobileNet1D, self).__init__()\n",
    "        self.first_channel = 32\n",
    "        self.last_channel = 1280\n",
    "\n",
    "        self.interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # block = InvertedResidualBlock\n",
    "        block = InvertedResidual\n",
    "\n",
    "        # First layer\n",
    "        self.features = [\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(input_channels, self.first_channel, 3, 2, 1, bias=False),\n",
    "                nn.BatchNorm1d(self.first_channel),\n",
    "                nn.ReLU6(inplace=True),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Inverted residual blocks\n",
    "        for t, c, n, s in self.interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(\n",
    "                        block(self.first_channel, output_channel, s, expansion_factor=t)\n",
    "                    )\n",
    "                else:\n",
    "                    self.features.append(\n",
    "                        block(self.first_channel, output_channel, 1, expansion_factor=t)\n",
    "                    )\n",
    "                self.first_channel = output_channel\n",
    "\n",
    "        # Last layer\n",
    "        self.features.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(self.first_channel, self.last_channel, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm1d(self.last_channel),\n",
    "                nn.ReLU6(inplace=True),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.features = nn.ModuleList(self.features)\n",
    "\n",
    "    def forward(self, x, return_layers=True):\n",
    "        features = []\n",
    "        for i, block in enumerate(self.features):\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "        return x, [features[6], features[3], features[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d0163-ff56-45b2-9e5c-c9bb3c635c5c",
   "metadata": {},
   "source": [
    "# TransUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883710ab-6d49-4736-841d-c3678a873145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding1D(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding1D, self).__init__()\n",
    "        \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b27ff-ee3b-4afe-9fc4-5c16979cf05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding1D(nn.Module):\n",
    "    def __init__(self, feature_dim, patch_size, embed_dim, num_patches):\n",
    "        super(PatchEmbedding1D, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Linear(feature_dim * patch_size, embed_dim)\n",
    "        \n",
    "        # Define learnable positional embeddings for each patch position\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, feature_dim, reduced_length = x.shape\n",
    "        num_patches = reduced_length // self.patch_size\n",
    "        \n",
    "        assert reduced_length % self.patch_size == 0, \"Reduced length must be divisible by patch size\"\n",
    "\n",
    "        x = x.unfold(dimension=2, size=self.patch_size, step=self.patch_size)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, num_patches, -1)\n",
    "\n",
    "        x = self.projection(x)\n",
    "        x = x + self.positional_embeddings[:, :num_patches, :]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919924a0-2e00-431b-bdb6-d8268ad92afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder1D(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers):\n",
    "        super(TransformerEncoder1D, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding1D(embed_dim=embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=2048)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x) \n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197ab85-abd5-46ec-b6f7-6a50bf39cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(nn.Conv1d(in_planes, in_planes // 16, 1, bias=False),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv1d(in_planes // 16, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=in_channels*8, kernel_size=1)\n",
    "        self.ca = ChannelAttention(in_channels*8)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.conv2 = nn.Conv1d(in_channels=in_channels*8, out_channels=in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.ca(x) * x\n",
    "        x = self.sa(x) * x\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f256cce-5063-4d57-a784-adadf514d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, num_patches, embed_dim, num_heads, num_layers, num_classes, encoder_name=\"ResNet\", input_signal=1800):\n",
    "        super(TransUNet, self).__init__()\n",
    "        self.encoder_name = encoder_name\n",
    "        \n",
    "        if self.encoder_name == \"EfficientNet\":\n",
    "            self.encoder = EfficientNetB0(in_channels)\n",
    "            feature_dim = 1280\n",
    "        elif self.encoder_name == \"ResNet\":\n",
    "            self.encoder = ResNet18(block=BasicBlock, layers=[2, 2, 2, 2], in_channels=in_channels)\n",
    "            feature_dim = 512\n",
    "        elif self.encoder_name == \"MobileNet\":\n",
    "            self.encoder = MobileNet1D(input_channels=in_channels)\n",
    "            feature_dim = 1280\n",
    "        \n",
    "        self.patch_embedding = PatchEmbedding1D(feature_dim=feature_dim, patch_size=patch_size, embed_dim=embed_dim, num_patches=num_patches)\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder1D(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "\n",
    "        if self.encoder_name == \"EfficientNet\":\n",
    "            self.conv1 = nn.Conv1d(in_channels=num_patches, out_channels=40, kernel_size=1)\n",
    "            self.conv2 = nn.Conv1d(in_channels=80, out_channels=24, kernel_size=1)\n",
    "            self.conv3 = nn.Conv1d(in_channels=48, out_channels=32, kernel_size=1)\n",
    "            self.conv4 = nn.Conv1d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
    "            \n",
    "            self.up1 = nn.Upsample(size=math.ceil(input_signal/8), mode='linear', align_corners=True)\n",
    "            self.up2 = nn.Upsample(size=math.ceil(input_signal/4), mode='linear', align_corners=True)\n",
    "            self.up3 = nn.Upsample(size=math.ceil(input_signal/2), mode='linear', align_corners=True)\n",
    "            self.up4 = nn.Upsample(size=math.ceil(input_signal), mode='linear', align_corners=True)\n",
    "\n",
    "            # CBAM gate:\n",
    "            self.cbam1 = CBAM(in_channels=40)\n",
    "            self.cbam2 = CBAM(in_channels=24)\n",
    "            self.cbam3 = CBAM(in_channels=32)\n",
    "        elif self.encoder_name == \"ResNet\":\n",
    "            self.conv1 = nn.Conv1d(in_channels=num_patches, out_channels=128, kernel_size=1)\n",
    "            self.conv2 = nn.Conv1d(in_channels=256, out_channels=64, kernel_size=1)\n",
    "            self.conv3 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=1)\n",
    "            self.conv4 = nn.Conv1d(in_channels=128, out_channels=num_classes, kernel_size=1)\n",
    "            \n",
    "            self.up1 = nn.Upsample(size=math.ceil(input_signal/8), mode='linear', align_corners=True)\n",
    "            self.up2 = nn.Upsample(size=math.ceil(input_signal/4), mode='linear', align_corners=True)\n",
    "            self.up3 = nn.Upsample(size=math.ceil(input_signal/2), mode='linear', align_corners=True)\n",
    "            self.up4 = nn.Upsample(size=math.ceil(input_signal), mode='linear', align_corners=True)\n",
    "\n",
    "            # CBAM gate:\n",
    "            self.cbam1 = CBAM(in_channels=128)\n",
    "            self.cbam2 = CBAM(in_channels=64)\n",
    "            self.cbam3 = CBAM(in_channels=64)\n",
    "\n",
    "        elif self.encoder_name == \"MobileNet\":\n",
    "            self.conv1 = nn.Conv1d(in_channels=num_patches, out_channels=32, kernel_size=1)\n",
    "            self.conv2 = nn.Conv1d(in_channels=64, out_channels=24, kernel_size=1)\n",
    "            self.conv3 = nn.Conv1d(in_channels=48, out_channels=32, kernel_size=1)\n",
    "            self.conv4 = nn.Conv1d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
    "            \n",
    "            self.up1 = nn.Upsample(size=math.ceil(input_signal/8), mode='linear', align_corners=True)\n",
    "            self.up2 = nn.Upsample(size=math.ceil(input_signal/4), mode='linear', align_corners=True)\n",
    "            self.up3 = nn.Upsample(size=math.ceil(input_signal/2), mode='linear', align_corners=True)\n",
    "            self.up4 = nn.Upsample(size=math.ceil(input_signal), mode='linear', align_corners=True)\n",
    "\n",
    "            # CBAM gate:\n",
    "            self.cbam1 = CBAM(in_channels=32)\n",
    "            self.cbam2 = CBAM(in_channels=24)\n",
    "            self.cbam3 = CBAM(in_channels=32)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, features = self.encoder(x)\n",
    "        x = self.patch_embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        if self.encoder_name == \"EfficientNet\":\n",
    "            x = self.conv1(x)\n",
    "            x = self.up1(x)\n",
    "            tmp_features = self.cbam1(features[0])\n",
    "            x = torch.cat([x, tmp_features], dim=1)\n",
    "            x = self.conv2(x)\n",
    "            x = self.up2(x)\n",
    "            tmp_features = self.cbam2(features[1])\n",
    "            x = torch.cat([x, tmp_features], dim=1)\n",
    "            x = self.conv3(x)\n",
    "            x = self.up3(x)\n",
    "            tmp_features = self.cbam3(features[2])\n",
    "            x = torch.cat([x, tmp_features], dim=1)\n",
    "            x = self.conv4(x)\n",
    "            x = self.up4(x)\n",
    "            \n",
    "        elif self.encoder_name == \"ResNet\":\n",
    "            x = self.conv1(x)\n",
    "            x = self.up1(x)\n",
    "            tmp_features = self.cbam1(features[0])\n",
    "            x = torch.cat([x, tmp_features], dim=1)\n",
    "            x = self.conv2(x)\n",
    "            x = self.up2(x)\n",
    "            tmp_features = self.cbam2(features[1])\n",
    "            x = torch.cat([x, tmp_features], dim=1)\n",
    "            x = self.conv3(x)\n",
    "            x = self.up3(x)\n",
    "            tmp_features = self.cbam3(features[2])\n",
    "            x = torch.cat([x, tmp_features], dim=1)\n",
    "            x = self.conv4(x)\n",
    "            x = self.up4(x)\n",
    "\n",
    "        elif self.encoder_name == \"MobileNet\":\n",
    "            x = self.conv1(x)\n",
    "            x = self.up1(x)\n",
    "            tmp_features = self.cbam1(features[0])\n",
    "            x = torch.cat([x, tmp_features], dim=1)\n",
    "            x = self.conv2(x)\n",
    "            x = self.up2(x)\n",
    "            tmp_features = self.cbam2(features[1])\n",
    "            x = torch.cat([x, tmp_features], dim=1)\n",
    "            x = self.conv3(x)\n",
    "            x = self.up3(x)\n",
    "            tmp_features = self.cbam3(features[2])\n",
    "            x = torch.cat([x, tmp_features], dim=1)\n",
    "            x = self.conv4(x)\n",
    "            x = self.up4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d2344f-ce77-40d1-abbb-7a61c98d9c67",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7df8df-00b3-42d9-a0b7-7ee8aa3094cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, path, length=1800):\n",
    "        self.path = path\n",
    "        self.length = length\n",
    "        pass\n",
    "\n",
    "    def __segmentate_data(self, signal, peaks, labels):\n",
    "        start, end = 0, self.length\n",
    "        signals_tab, peaks_tab, labels_tab, peaks_idx = [], [], [], []\n",
    "        while end < signal.shape[0] and start < signal.shape[0]:\n",
    "            signals_tab.append(signal[start:end])\n",
    "            peaks_tab.append(peaks[start:end])\n",
    "            labels_tab.append(labels[start:end])\n",
    "\n",
    "            idx = np.where(peaks_tab[-1] == 1)[0]\n",
    "            if len(idx) == 0:\n",
    "                tmp = 0\n",
    "            else:\n",
    "                tmp = idx[-1]\n",
    "            peaks_idx.append(tmp)\n",
    "            if start == (start + tmp):\n",
    "                start = start + self.length\n",
    "                end = start + self.length\n",
    "            else:\n",
    "                start += tmp\n",
    "                end = start + self.length\n",
    "        return (np.array(signals_tab), np.array(peaks_tab), np.array(labels_tab), peaks_idx,)\n",
    "\n",
    "    def __read_file(self, file):\n",
    "        file_path = os.path.join(self.path, file)\n",
    "\n",
    "        record = wfdb.rdrecord(file_path)\n",
    "        annotation = wfdb.rdann(file_path, \"atr\")\n",
    "\n",
    "        size = record.p_signal.shape[0]\n",
    "\n",
    "        file_peaks = np.zeros(size)  # coords of R-peaks\n",
    "        file_labels = np.zeros(size)  # labels of each time step\n",
    "        tmp_sample, tmp_symbol = [], []  # coors and symbols of R-peaks\n",
    "\n",
    "        # get rid of noninformative samples\n",
    "        for i in range(len(annotation.symbol)):\n",
    "            # idk if that list is the same for other data\n",
    "            if annotation.symbol[i] not in [\"+\", \"~\", \"x\", \"|\"]:\n",
    "                tmp_sample.append(annotation.sample[i])\n",
    "                tmp_symbol.append(annotation.symbol[i])\n",
    "                file_peaks[annotation.sample[i]] = 1\n",
    "\n",
    "        # creating labels list based on the\n",
    "        for i in range(len(tmp_symbol) - 1):\n",
    "            if tmp_symbol[i] in [\"N\", \"L\", \"R\", \"e\", \"j\"]:\n",
    "                file_labels[tmp_sample[i] : tmp_sample[i + 1]] = 0\n",
    "            elif tmp_symbol[i] in [\"A\", \"a\", \"J\", \"S\"]:\n",
    "                file_labels[tmp_sample[i] : tmp_sample[i + 1]] = 1\n",
    "            elif tmp_symbol[i] in [\"V\", \"E\"]:\n",
    "                file_labels[tmp_sample[i] : tmp_sample[i + 1]] = 2\n",
    "            elif tmp_symbol[i] in [\"F\"]:\n",
    "                file_labels[tmp_sample[i] : tmp_sample[i + 1]] = 3\n",
    "            elif tmp_symbol[i] in [\"/\", \"f\", \"Q\"]:\n",
    "                file_labels[tmp_sample[i] : tmp_sample[i + 1]] = 4\n",
    "\n",
    "        # get rid of time steps behind first classified R-peak, and after the last one\n",
    "        file_signals = record.p_signal[tmp_sample[0] : tmp_sample[-1]]\n",
    "        file_labels = file_labels[tmp_sample[0] : tmp_sample[-1]]\n",
    "        file_peaks = file_peaks[tmp_sample[0] : tmp_sample[-1]]\n",
    "\n",
    "        # segmentate signals, labels, peaks into n sec segments\n",
    "        segmented_signals, segmented_peaks, segmented_outputs, peak_idx = (\n",
    "            self.__segmentate_data(file_signals, file_peaks, file_labels)\n",
    "        )\n",
    "\n",
    "        return segmented_signals, segmented_peaks, segmented_outputs, peak_idx\n",
    "        \n",
    "    def read_dataset(self):\n",
    "        f = open(f\"{self.path}/RECORDS\", \"r\")\n",
    "        files = f.read().replace(\"\\n\", \" \").split()\n",
    "\n",
    "        peaks, targets, inputs, peaks_idx = [], [], [], []\n",
    "\n",
    "        for file in files:\n",
    "            segmented_signals, segmented_peaks, segmented_outputs, peak_idx = self.__read_file(file)\n",
    "            \n",
    "            peaks.extend(segmented_peaks)\n",
    "            inputs.extend(segmented_signals)\n",
    "            targets.extend(segmented_outputs)\n",
    "            peaks_idx.extend(peak_idx)\n",
    "\n",
    "        return np.array(inputs), np.array(targets), np.array(peaks), np.array(peaks_idx)\n",
    "        \n",
    "    \n",
    "    def read_dataset_patients(self, num_fold):\n",
    "        random.seed(42)\n",
    "        f = open(f\"{self.path}/RECORDS\", \"r\")\n",
    "        files = f.read().replace(\"\\n\", \" \").split()\n",
    "        random.shuffle(files)\n",
    "        folds = np.array_split(files, 5)\n",
    "\n",
    "        train_files, test_files = [], []\n",
    "        for i in range(len(folds)):\n",
    "            if i == num_fold:\n",
    "                test_files.extend(folds[i])\n",
    "            else:\n",
    "                train_files.extend(folds[i])\n",
    "\n",
    "        peaks_train, targets_train, inputs_train, peaks_idx_train = [], [], [], []\n",
    "        peaks_test, targets_test, inputs_test, peaks_idx_test = [], [], [], []\n",
    "\n",
    "        for file in files:\n",
    "            segmented_signals, segmented_peaks, segmented_outputs, peak_idx = self.__read_file(file)\n",
    "\n",
    "            if file in train_files:\n",
    "                peaks_train.extend(segmented_peaks)\n",
    "                inputs_train.extend(segmented_signals)\n",
    "                targets_train.extend(segmented_outputs)\n",
    "                peaks_idx_train.extend(peak_idx)\n",
    "            elif file in test_files:\n",
    "                peaks_test.extend(segmented_peaks)\n",
    "                inputs_test.extend(segmented_signals)\n",
    "                targets_test.extend(segmented_outputs)\n",
    "                peaks_idx_test.extend(peak_idx)\n",
    "                \n",
    "        X_train = np.array(inputs_train)\n",
    "        X_test = np.array(inputs_test)\n",
    "        y_train = np.array(targets_train)\n",
    "        y_test = np.array(targets_test)\n",
    "        peaks_train = np.array(peaks_train)\n",
    "        peaks_test = np.array(peaks_test)\n",
    "        peaks_idx_train = np.array(peaks_idx_train)\n",
    "        peaks_idx_test = np.array(peaks_idx_test)\n",
    "\n",
    "        return (X_train.transpose(0, 2, 1), X_test.transpose(0, 2, 1), peaks_train, peaks_test, y_train, y_test, peaks_idx_train, peaks_idx_test,)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533e67a-ca43-492a-a1d0-b7c337725b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_data(targets, preds, peaks):\n",
    "    targets = np.array(targets.cpu())\n",
    "    preds = np.array(preds.cpu())\n",
    "    peaks = np.array(peaks.cpu())\n",
    "    indices_of_ones = [np.where(row == 1)[0] for row in peaks]\n",
    "    categorized_preds_most, categorized_targets = [], []\n",
    "    \n",
    "    for i in range(len(peaks)):\n",
    "        tmp_preds_most, tmp_targets = [], []\n",
    "        for j in range(len(indices_of_ones[i])):\n",
    "            if j == 0 and indices_of_ones[i][j] != 0:\n",
    "                counter = Counter(preds[i][0 : indices_of_ones[i][0]])\n",
    "                most_common_number, count = counter.most_common(1)[0]\n",
    "                tmp_preds_most.append(most_common_number)\n",
    "                \n",
    "                counter = Counter(targets[i][0 : indices_of_ones[i][0]])\n",
    "                counter_target, count = counter.most_common(1)[0]\n",
    "            elif j == len(indices_of_ones[i]) - 1:\n",
    "                counter = Counter(preds[i][indices_of_ones[i][-1] : len(peaks[i])])\n",
    "                most_common_number, count = counter.most_common(1)[0]\n",
    "                tmp_preds_most.append(most_common_number)\n",
    "                \n",
    "                counter = Counter(targets[i][indices_of_ones[i][-1] : len(peaks[i])])\n",
    "                counter_target, count = counter.most_common(1)[0]\n",
    "            else:\n",
    "                counter = Counter(preds[i][indices_of_ones[i][j] : indices_of_ones[i][j + 1]])\n",
    "                most_common_number, count = counter.most_common(1)[0]\n",
    "                tmp_preds_most.append(most_common_number)\n",
    "                \n",
    "                counter = Counter(targets[i][indices_of_ones[i][j] : indices_of_ones[i][j + 1]])\n",
    "                counter_target, count = counter.most_common(1)[0]\n",
    "            tmp_targets.append(counter_target)\n",
    "        \n",
    "        categorized_preds_most.append(tmp_preds_most)\n",
    "        categorized_targets.append(tmp_targets)\n",
    "    return categorized_preds_most, categorized_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf26e7-182d-418c-8741-57241d388eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, peaks, peaks_idx):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.peaks = peaks\n",
    "        self.peaks_idx = peaks_idx\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self.inputs[index]\n",
    "        target = self.targets[index]\n",
    "        peak = self.peaks[index]\n",
    "        peak_idx = self.peaks_idx[index]\n",
    "\n",
    "        input = self.transform(input)\n",
    "\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "        peak = torch.tensor(peak, dtype=torch.long)\n",
    "        peak_idx = torch.tensor(peak_idx, dtype=torch.long)\n",
    "\n",
    "        return input[0], target, peak, peak_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98d7fb-4ce6-496a-bf14-3b372bad1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrices(preds, targets, num_classes, end_idxs=None):\n",
    "    accuracy, precision, recall, f1 = [], [], [], []\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    \n",
    "    for i in range(len(targets)):\n",
    "        if end_idxs is None:\n",
    "            tmp_true = targets[i]\n",
    "            tmp_pred = preds[i]\n",
    "        else:\n",
    "            if end_idxs[i].item() == 0:\n",
    "                continue\n",
    "            else:\n",
    "                tmp_true = targets[i][: -1]\n",
    "                tmp_pred = preds[i][: -1]\n",
    "\n",
    "        accuracy.append(accuracy_score(tmp_true,tmp_pred))\n",
    "        precision.append(precision_score(tmp_true,tmp_pred, zero_division=0, average=\"weighted\"))\n",
    "        recall.append(recall_score(tmp_true,tmp_pred, zero_division=0, average=\"weighted\"))\n",
    "        f1.append(f1_score(tmp_true,tmp_pred, zero_division=0, average=\"weighted\"))\n",
    "        cm += confusion_matrix(tmp_true,tmp_pred, labels=np.arange(num_classes))\n",
    "        \n",
    "    return (np.nanmean(accuracy), np.nanmean(precision), np.nanmean(recall), np.nanmean(f1), cm,)\n",
    "\n",
    "\n",
    "def iou(pred, target, num_classes, epsilon=1e-6):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        union = (pred_inds | target_inds).sum().float()\n",
    "        if union == 0:\n",
    "            ious.append(np.nan)  # Only happens if both pred and target are empty for this class\n",
    "        else:\n",
    "            ious.append((intersection / (union + epsilon)).item())\n",
    "    \n",
    "    return np.nanmean(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94271a-afaf-4e25-8b49-a6151415a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8732644-f80f-4ba7-97a5-99dfa8d48ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, data_loader, device, num_classes, num_channels=2):\n",
    "    model.eval()\n",
    "\n",
    "    iou_score = []\n",
    "    acc_tab, prec_tab, rec_tab, f1_tab = [], [], [], []\n",
    "    cm_tab = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, peaks, end_idxs in data_loader:\n",
    "            inputs = inputs.to(torch.float32).to(device)\n",
    "            targets = targets.to(device)\n",
    "            peaks = peaks.to(device)\n",
    "            end_idxs = end_idxs.to(device)\n",
    "\n",
    "            batch_outputs = model(inputs)\n",
    "\n",
    "            outputs = batch_outputs.permute(0,2,1).contiguous()\n",
    "            outputs = outputs.view(-1, num_classes)\n",
    "            target = targets.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss+=loss.item()\n",
    "\n",
    "            preds = torch.argmax(batch_outputs, dim=1)\n",
    "\n",
    "            # continuous normal\n",
    "            iou_score.append(iou(preds, targets, num_channels))\n",
    "\n",
    "            # categorized normal\n",
    "            categorized_preds, categorized_targets = categorize_data(targets, preds, peaks)\n",
    "            acc, prec, rec, f1, cm = calculate_metrices(preds=categorized_preds, targets=categorized_targets, num_classes=num_classes, end_idxs=end_idxs)\n",
    "            acc_tab.append(acc)\n",
    "            prec_tab.append(prec)\n",
    "            rec_tab.append(rec)\n",
    "            f1_tab.append(f1)\n",
    "            cm_tab += cm\n",
    "\n",
    "    print(\"Testing!\")\n",
    "    print(f\"Loss value: {running_loss/len(data_loader)}, Accuracy: {np.nanmean(acc_tab)}\")\n",
    "\n",
    "    return_list = [running_loss/len(data_loader), np.nanmean(iou_score),\n",
    "                   np.nanmean(acc_tab), np.nanmean(prec_tab), np.nanmean(rec_tab), np.nanmean(f1_tab), cm_tab]\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dcbfdf-cd72-4304-8fb6-535252b3fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, data_loader, device, num_classes, num_channels=2):\n",
    "    model.train()\n",
    "\n",
    "    iou_score = []\n",
    "    acc_tab, prec_tab, rec_tab, f1_tab = [], [], [], []\n",
    "    cm_tab = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for inputs, targets, peaks, end_idxs in data_loader:\n",
    "        inputs = inputs.to(torch.float32).to(device)\n",
    "        targets = targets.to(device)\n",
    "        peaks = peaks.to(device)\n",
    "        end_idxs = end_idxs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_outputs = model(inputs)\n",
    "\n",
    "        outputs = batch_outputs.permute(0,2,1).contiguous()\n",
    "        outputs = outputs.view(-1, num_classes)\n",
    "        target = targets.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = torch.argmax(batch_outputs, dim=1)\n",
    "\n",
    "        # continuous normal\n",
    "        iou_score.append(iou(preds, targets, num_channels))\n",
    "\n",
    "        # categorized normal\n",
    "        categorized_preds, categorized_targets = categorize_data(targets, preds, peaks)\n",
    "        acc, prec, rec, f1, cm = calculate_metrices(preds=categorized_preds, targets=categorized_targets, num_classes=num_classes, end_idxs=end_idxs)\n",
    "        acc_tab.append(acc)\n",
    "        prec_tab.append(prec)\n",
    "        rec_tab.append(rec)\n",
    "        f1_tab.append(f1)\n",
    "        cm_tab += cm\n",
    "\n",
    "        \n",
    "    print(\"Training!\")\n",
    "    print(f\"Loss value: {running_loss/len(data_loader)}, Accuracy: {np.nanmean(acc_tab)}\")\n",
    "    return_list = [running_loss/len(data_loader), np.nanmean(iou_score),\n",
    "                   np.nanmean(acc_tab), np.nanmean(prec_tab), np.nanmean(rec_tab), np.nanmean(f1_tab),cm_tab]\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde05f3-5944-458b-959e-f491a86a1254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c29dd87-5a00-4133-b838-d60ffc2dba79",
   "metadata": {},
   "source": [
    "# Testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654667-f684-4e1d-9e10-2c1d6646ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(train_results, test_results, epochs, encoder_name, evaluation_type, path_to_save, num_folds=5):\n",
    "    test_loss, test_iou, test_acc, test_prec, test_rec, test_f1 = np.zeros(epochs), np.zeros(epochs), np.zeros(epochs), np.zeros(epochs), np.zeros(epochs), np.zeros(epochs)\n",
    "    train_loss, train_iou, train_acc, train_prec, train_rec, train_f1 = np.zeros(epochs), np.zeros(epochs), np.zeros(epochs), np.zeros(epochs), np.zeros(epochs), np.zeros(epochs)\n",
    "    train_cm, test_cm = [np.zeros((5, 5), dtype=np.float32) for _ in range(epochs)], [np.zeros((5, 5), dtype=np.float32) for _ in range(epochs)]\n",
    "    for i in range(len(train_results)):\n",
    "        # train part\n",
    "        for j in range(len(train_results[i])):\n",
    "            train_loss[j] += train_results[i][j][0]\n",
    "            train_iou[j] += train_results[i][j][1]\n",
    "            train_acc[j] += train_results[i][j][2]\n",
    "            train_prec[j] += train_results[i][j][3]\n",
    "            train_rec[j] += train_results[i][j][4]\n",
    "            train_f1[j] += train_results[i][j][5]\n",
    "            train_cm[j] += train_results[i][j][6]\n",
    "\n",
    "        # test part\n",
    "        for j in range(len(train_results[i])):\n",
    "            test_loss[j] += test_results[i][j][0]\n",
    "            test_iou[j] += test_results[i][j][1]\n",
    "            test_acc[j] += test_results[i][j][2]\n",
    "            test_prec[j] += test_results[i][j][3]\n",
    "            test_rec[j] += test_results[i][j][4]\n",
    "            test_f1[j] += test_results[i][j][5]\n",
    "            test_cm[j] += test_results[i][j][6]\n",
    "\n",
    "    train_loss /= num_folds\n",
    "    train_iou /= num_folds\n",
    "    train_acc /= num_folds\n",
    "    train_prec /= num_folds\n",
    "    train_rec /= num_folds\n",
    "    train_f1 /= num_folds\n",
    "    for i in train_cm:\n",
    "        i /= num_folds\n",
    "\n",
    "    test_loss /= num_folds\n",
    "    test_iou /= num_folds\n",
    "    test_acc /= num_folds\n",
    "    test_prec /= num_folds\n",
    "    test_rec /= num_folds\n",
    "    test_f1 /= num_folds\n",
    "    for i in test_cm:\n",
    "        i /= num_folds\n",
    "\n",
    "    result_list = {\"train_loss\": train_loss, \"train_iou\": train_iou, \"train_acc\": train_acc, \"train_prec\": train_prec, \"train_rec\": train_rec, \"train_f1\": train_f1, \"train_cm\": train_cm,\n",
    "                  \"test_loss\": test_loss, \"test_iou\": test_iou, \"test_acc\": test_acc, \"test_prec\": test_prec, \"test_rec\": test_rec, \"test_f1\": test_f1, \"test_cm\": test_cm}\n",
    "\n",
    "    tmp_path = os.path.join(path_to_save, f\"{encoder_name}_{evaluation_type}\")\n",
    "    os.mkdir(tmp_path)\n",
    "\n",
    "    for key, value in result_list.items():\n",
    "        if key in [\"train_cm\", \"test_cm\"]:\n",
    "            file_path = os.path.join(tmp_path, f\"{key}.npz\")\n",
    "            np.savez(file_path, *value)\n",
    "        else:\n",
    "            file_path = os.path.join(tmp_path, f\"{key}.txt\")\n",
    "            with open(file_path, \"w\") as file:\n",
    "                for i in value:\n",
    "                    file.write(f\"{i};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaba786-7f2e-4da6-bebd-018622dc532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_patients(data_path, encoder_name, device, path_to_save, length=1800, batch_size=64, num_classes=5, learning_rate=1e-4, epochs=100):\n",
    "    dataset = Dataset(path=data_path, length=length)\n",
    "    inputs, targets, peaks, peaks_idx = dataset.read_dataset()\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    folds_train, folds_test = [], []\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(inputs)):\n",
    "        X_train, X_test = inputs[train_index], inputs[test_index]\n",
    "        y_train, y_test = targets[train_index], targets[test_index]\n",
    "        peaks_train, peaks_test = peaks[train_index], peaks[test_index]\n",
    "        peaks_idx_train, peaks_idx_test = peaks_idx[train_index], peaks_idx[test_index]\n",
    "        \n",
    "        train_dataset = ECGDataset(X_train.transpose(0, 2, 1), y_train, peaks_train, peaks_idx_train)\n",
    "        test_dataset = ECGDataset(X_test.transpose(0, 2, 1), y_test, peaks_test, peaks_idx_test)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        model = TransUNet(in_channels=2, patch_size=1, num_patches=57, embed_dim=256, num_heads=8,\n",
    "                          num_layers=4, num_classes=5, encoder_name=encoder_name, input_signal=length).to(device)\n",
    "\n",
    "        criterion = FocalLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_tab, test_tab = [], []\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}\")\n",
    "            train_list = train_model(model=model, criterion=criterion, optimizer=optimizer, data_loader=train_loader, device=device, num_classes=num_classes)\n",
    "            test_list = test_model(model=model, criterion=criterion, data_loader=test_loader, device=device, num_classes=num_classes)\n",
    "            train_tab.append(train_list)\n",
    "            test_tab.append(test_list)\n",
    "\n",
    "        folds_train.append(train_tab)\n",
    "        folds_test.append(test_tab)\n",
    "\n",
    "    make_results(train_results=folds_train, test_results=folds_test, epochs=epochs, encoder_name=encoder_name, \n",
    "                 evaluation_type=\"Mix\", path_to_save=path_to_save, num_folds=5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c26449-9ae4-471e-a9a9-cff61e6fdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divided_patients(data_path, encoder_name, device, path_to_save, length=1800, batch_size=64, num_classes=5, learning_rate=1e-4, epochs=100):\n",
    "    folds_train, folds_test = [], []\n",
    "    for i in range(5):\n",
    "        dataset = Dataset(path=data_path, length=length)\n",
    "        X_train, X_test, peaks_train, peaks_test, y_train, y_test, peaks_idx_train, peaks_idx_test = dataset.read_dataset_patients(num_fold=i)\n",
    "        \n",
    "        train_dataset = ECGDataset(X_train, y_train, peaks_train, peaks_idx_train)\n",
    "        test_dataset = ECGDataset(X_test, y_test, peaks_test, peaks_idx_test)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        model = TransUNet(in_channels=2, patch_size=1, num_patches=57, embed_dim=256, num_heads=8,\n",
    "                          num_layers=4, num_classes=5, encoder_name=encoder_name, input_signal=length).to(device)\n",
    "\n",
    "        criterion = FocalLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_tab, test_tab = [], []\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}\")\n",
    "            train_list = train_model(model=model, criterion=criterion, optimizer=optimizer, data_loader=train_loader, device=device, num_classes=num_classes)\n",
    "            test_list = test_model(model=model, criterion=criterion, data_loader=test_loader, device=device, num_classes=num_classes)\n",
    "            train_tab.append(train_list)\n",
    "            test_tab.append(test_list)\n",
    "\n",
    "        folds_train.append(train_tab)\n",
    "        folds_test.append(test_tab)\n",
    "\n",
    "    make_results(train_results=folds_train, test_results=folds_test, epochs=epochs, encoder_name=encoder_name, \n",
    "                 evaluation_type=\"Divided\", path_to_save=path_to_save, num_folds=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be810484-193a-4804-8e8e-041fad97252d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoders = [\"EfficientNet\", \"ResNet\", \"MobileNet\"]\n",
    "methods = [\"Divided\", \"Mix\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = os.path.join(os.getcwd(), \"mit-bih-arrhythmia-database-1.0.0\")\n",
    "path_to_save = os.path.join(os.getcwd(), \"Results\")\n",
    "num_classes = 5\n",
    "in_channels = 2\n",
    "in_length = 1800\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "epochs = 200\n",
    "\n",
    "os.mkdir(path_to_save)\n",
    "for encoder in encoders:\n",
    "    for method in methods:\n",
    "        if method == \"Mix\":\n",
    "            mixed_patients(data_path=path, encoder_name=encoder, device=device, path_to_save=path_to_save, epochs=epochs)\n",
    "        elif method == \"Divided\":\n",
    "            divided_patients(data_path=path, encoder_name=encoder, device=device, path_to_save=path_to_save, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d3ec2-629c-4748-a1df-f09c06ec3612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b10ab-d65f-4366-ad6f-cb248010c216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95635d58-f442-43f2-aaef-56558ddc03a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cc22e-49d0-4309-9738-77c851fe4fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
